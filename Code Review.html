<!DOCTYPE html>
<html>

<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>Code Review</title>
  <link rel="stylesheet" href="https://stackedit.io/style.css" />
</head>

<body class="stackedit">
  <div class="stackedit__left">
    <div class="stackedit__toc">
      
<ul>
<li><a href="#cs-499-code-review-video---complete-transcript-revised">CS 499 Code Review Video - Complete Transcript (Revised)</a>
<ul>
<li><a href="#introduction">Introduction</a></li>
<li><a href="#category-one-software-design-and-engineering">CATEGORY ONE: Software Design and Engineering</a></li>
<li><a href="#category-two-algorithms-and-data-structures">CATEGORY TWO: Algorithms and Data Structures</a></li>
<li><a href="#category-three-databases">CATEGORY THREE: Databases</a></li>
<li><a href="#conclusion">Conclusion</a></li>
</ul>
</li>
</ul>

    </div>
  </div>
  <div class="stackedit__right">
    <div class="stackedit__html">
      <h1 id="cs-499-code-review-video---complete-transcript-revised">CS 499 Code Review Video - Complete Transcript (Revised)</h1>
<h2 id="introduction"><strong>Introduction</strong></h2>
<p>Hello, my name is Eriko Maledhi, and welcome to my CS 499 code review for Milestone One. I’ve been enrolled in the Computer Science program at Southern New Hampshire University for about a year, and throughout this time, I’ve progressed from  programming courses to  topics in software engineering, algorithms, databases, and system architecture.</p>
<p>Today, I’ll be walking through two artifacts from my coursework: the CS 340 Grazioso Salvare Dashboard and the CS 360 Weight Tracker application. I’ll analyze their current functionality, identify areas for improvement, and explain my enhancement plans across three categories: Software Design and Engineering, Algorithms and Data Structures, and Databases.</p>
<h2 id="category-one-software-design-and-engineering"><strong>CATEGORY ONE: Software Design and Engineering</strong></h2>
<h3 id="artifact-cs-340-grazioso-salvare-animal-rescue-dashboard"><strong>Artifact: CS 340 Grazioso Salvare Animal Rescue Dashboard</strong></h3>
<h3 id="existing-code-functionality"><strong>Existing Code Functionality</strong></h3>
<p>The first artifact I’ll be reviewing is the Grazioso Salvare Animal Rescue Dashboard, which I developed in CS 340. This application was designed for Grazioso Salvare, a company specializing in training rescue animals, to help identify suitable dogs for different rescue operations.</p>
<p>The application consists of two main components. First, we have the Python-based CRUD module called animal_shelter.py. This module uses PyMongo to interact with our MongoDB database and provides basic Create, Read, Update, and Delete operations for managing animal records. The methods are straightforward, we have a create method to insert new animals, a read method to query the database, an update method to modify existing records, and a delete method to remove animals.</p>
<p>Second, we have the Dash-based dashboard interface, which is implemented in a Jupyter notebook. This dashboard provides the user interface and includes several interactive features. There’s a data table that displays animal records with sorting and pagination capabilities. Users can interact with radio button filters that allow them to filter animals by rescue type - Water Rescue, Mountain Rescue, Disaster Rescue, or show all animals. The dashboard also includes a pie chart that visualizes the distribution of dog breeds in the filtered dataset, and an interactive geolocation map using Plotly that shows where each animal is located. All of these components update dynamically when users select different filter options.</p>
<p>The current architecture follows a monolithic pattern where the Dash framework handles both the backend logic and frontend rendering within a single Python application. The application connects directly to MongoDB using PyMongo, processes the data in Python, and renders the interface using Dash components.</p>
<p>So in summary, this code allows users to quickly identify dogs that match specific rescue operation requirements by filtering through a database of over 10,000 animal records, viewing their characteristics, and seeing their geographic locations.</p>
<h3 id="code-analysis"><strong>Code Analysis</strong></h3>
<p>Now I’ll critically analyze this code to identify areas that need improvement.</p>
<p>Starting with structure, the code is functional, but there are significant architectural concerns. The biggest issue is the lack of separation of concerns. In the current implementation, business logic, data access, and presentation are all coupled within the Jupyter notebook. There’s no clear modular routing structure - everything is handled in essentially one or two files.</p>
<p>The current implementation doesn’t use reusable component libraries. The Dash components are functional but not designed for reusability across different projects or even different parts of the same application. The main dashboard file combines too many responsibilities - it’s handling data fetching, filtering logic, chart generation, map rendering, and UI layout all in one place.</p>
<p>Now for the most critical issues - security. There’s minimal input validation. If a user or external system provides malformed data, the application could crash or behave unexpectedly. More importantly, from a security perspective, there’s no authentication mechanism whatsoever. Anyone who can access the application can view and potentially modify data. There’s no authorization controls - all users have the same access level. There’s no distinction between a regular user who should only view data and an administrator who might need to modify records.</p>
<p>The database credentials may be exposed directly in the code, which is a security risk. There’s no implementation of secure coding practices like input sanitization, which leaves the application vulnerable to injection attacks. If the MongoDB connection fails, there’s minimal error handling to gracefully manage that situation and inform the user.</p>
<p>There’s no environment configuration management - configuration values are hardcoded rather than externalized.</p>
<h3 id="planned-enhancements"><strong>Planned Enhancements</strong></h3>
<p>To address all these findings, I plan to completely re-architect this application by transforming it from a Python/Dash monolithic application into a full-stack MERN application - that’s MongoDB, Express.js, React, and Node.js.</p>
<p>For the backend architecture with Node.js and Express.js, I’ll transform the simple Python CRUD module into a  RESTful API. This will include an Express.js server with a modular routing structure. Instead of everything in one file, I’ll separate concerns into routes, controllers, services, and models. Routes will define the endpoints, controllers will handle request and response logic, services will contain business logic, and models will define our data schemas.</p>
<p>I’ll use Mongoose ODM for MongoDB integration. Mongoose provides schema validation at the database layer, which adds an extra layer of data integrity. I’ll implement comprehensive error handling with custom error classes. This means instead of the application crashing, it will catch errors, log them appropriately, and return meaningful error messages to the client. Most importantly, I’ll implement JWT-based authentication for administrative operations. JSON Web Tokens will allow users to securely log in, and the server will verify their identity before allowing access to protected resources.</p>
<p>For the frontend architecture with React.js, I’ll rebuild the Dash interface as a React Single-Page Application. This includes component-based architecture with reusable UI elements. Instead of one monolithic interface, I’ll create modular components like an AnimalTable component, a FilterPanel component, a BreedChart component, and a LocationMap component. Each can be developed, tested, and maintained independently.</p>
<p>I’ll implement React Router for client-side navigation, enabling smooth transitions between different views without full page reloads. I’ll create custom hooks for data fetching and business logic. For example, I might create a useAnimals hook that handles fetching animal data, caching it, and providing loading and error states. For styling, I’ll use Material-UI or Tailwind CSS for professional styling. I’ll also add form validation with real-time feedback, so users know immediately if they’ve entered invalid data.</p>
<p>Regarding security enhancements, I’ll implement JWT-based authentication where users must log in with credentials. I’ll implement input validation and sanitization at multiple layers - client-side for immediate user feedback, server-side to protect against malicious requests, and database-level with Mongoose schemas. I’ll ensure secure password handling using bcrypt for hashing - passwords will never be stored in plain text.</p>
<p>For professional development practices, I’ll implement  Prettier for code quality and consistency. I’ll follow a proper Git workflow with meaningful commit messages, comprehensive README documentation with setup instructions, API endpoint descriptions, and architectural diagrams.</p>
<h2 id="category-two-algorithms-and-data-structures"><strong>CATEGORY TWO: Algorithms and Data Structures</strong></h2>
<h3 id="artifact-cs-340-grazioso-salvare-dashboard"><strong>Artifact: CS 340 Grazioso Salvare Dashboard</strong></h3>
<h3 id="existing-code-functionality-1"><strong>Existing Code Functionality</strong></h3>
<p>Now I’ll examine the same Grazioso Salvare Dashboard artifact from an algorithms and data structures perspective.</p>
<p>The current implementation handles a dataset of over 10,000 animal records. When we look at how the application performs search, filter, and sort operations, we see significant performance limitations.</p>
<p>All search operations use linear search through Python lists with O(n) time complexity. For example, when a user clicks on a specific animal to view its details, the system iterates through the entire dataset sequentially until it finds the matching animal ID. With 10,000 records, this means potentially 10,000 comparison operations for a single lookup.</p>
<p>Similarly, filtering by breed, age, or rescue type requires iterating through the entire dataset every single time. If a user wants to see all Labrador Retrievers, the system checks every single animal record one by one. Sorting uses Python’s built-in  algorithm, which is efficient at O(n log n), but the implementation doesn’t take advantage of any custom optimization for the specific use case. Every time a user sorts the table, we’re sorting the entire dataset even if they only want to see the top results.</p>
<p>There’s also no caching mechanism. Every query, even repeated queries for the exact same data, hits the database. If a user switches between Water Rescue and Mountain Rescue filters repeatedly, we’re fetching and processing the entire dataset each time instead of caching the results. Finally, beyond MongoDB’s default underscore id index, there’s no indexing strategy optimized for our specific query patterns.</p>
<h3 id="code-analysis-1"><strong>Code Analysis</strong></h3>
<p>With 10,000 plus records, linear search for animal ID lookup is highly inefficient. In the worst case, finding an animal at the end of the dataset requires examining all 10,000 records - that’s O(n) time complexity. Every filter operation uses a linear scan. Every time someone filters by breed, we iterate through all records. This is wasteful when the same filter might be applied repeatedly.</p>
<p>We’re storing data in basic lists and dictionaries without considering the performance implications of our choice of data structures. There’s no use of tree structures for ordered data or hash-based indexing for categorical data like breed. For example, when users want to see only the top 10 youngest dogs, we’re sorting the entire 10,000-record dataset and then taking the first 10 results. This is O(n log n) when we could use a selection algorithm that’s O(n).</p>
<p>Let me quantify the problem. If we assume each comparison operation takes 1 microsecond, linear search for one animal ID takes up to 10,000 microseconds, which is 10 milliseconds. Filtering by breed across 10,000 animals takes 10 milliseconds per filter operation. With repeated filters and no caching, this time multiplies for each interaction. This might seem fast, but as the dataset grows to 50,000 or 100,000 animals, these operations become noticeably slow.</p>
<h3 id="planned-enhancements-1"><strong>Planned Enhancements</strong></h3>
<p>To address these performance issues, I will implement four key algorithmic improvements.</p>
<p><strong>Enhancement 1 is a Binary Search Tree for Animal ID Lookup.</strong> The purpose is to enable efficient O(log n) average-case search time instead of O(n) linear search. The use case is when a user clicks on an animal in the table to view detailed information - the system must quickly retrieve that specific animal’s complete record using its ID.</p>
<p>I’ll create a Binary Search Tree data structure where each node contains an animal object. The tree will be organized by animal ID, with smaller IDs in the left subtree and larger IDs in the right subtree.</p>
<p>For searching, I’ll have a search function that takes an animalId and calls searchRecursive starting from the root. I</p>
<p>Time Complexity Analysis: Insert is O(log n) average case and O(n) worst case if the tree becomes unbalanced. Search is O(log n) average case and O(n) worst case. Space complexity is O(n) to store all nodes. For a balanced tree with 10,000 animals, this reduces search operations from 10,000 comparisons with linear search to approximately 13 to 14 comparisons, since log base 2 of 10,000 is about 13.3. That’s roughly a 700 times improvement in the number of comparisons.</p>
<p><strong>Enhancement 2 is a Hash Table for Breed Lookup.</strong> The purpose is to achieve O(1) average-case lookup time for finding all animals of a specific breed. The use case is when users filter the dashboard by breed, such as showing all Labrador Retrievers - the system must instantly retrieve all matching animals.</p>
<p>The data structure will be a hash table where the key is the breed name and the value is an array of animal objects of that breed. For example, Labrador Retriever maps to an array of animal objects, German Shepherd maps to another array, Beagle maps to another array, and so on.</p>
<p>The implementation will have a class called BreedHashTable with a buildIndex function. This function creates an empty index object and iterates through all animals. For each animal, it gets the breed. If the breed is not already in the index, it creates an empty array for that breed. Then it appends the animal to that breed’s array. Finally, it returns the index. There’s also a getByBreed function. If the breed exists in the index, it returns the array of animals. Otherwise, it returns an empty array.</p>
<p>Time Complexity Analysis: Building the index is O(n) because we iterate through all animals once. Inserting a new animal is O(1) average case. Looking up by breed is O(1) average case. Space complexity is O(n plus k) where k is the number of unique breeds. For 10,000 animals with approximately 50 unique breeds, looking up all Labrador Retrievers goes from O(10,000) comparisons to O(1) hash lookup plus returning the resulting array. If there are 500 Labradors, we return them immediately instead of scanning all 10,000 records.</p>
<p>I will create comprehensive benchmarks comparing the old and new implementations. I’ll measure ID lookup with 10,000 operations comparing Linear O(n) versus BST O(log n). I’ll measure breed filter with 1,000 filter operations comparing Linear O(n) versus Hash O(1).</p>
<h2 id="category-three-databases"><strong>CATEGORY THREE: Databases</strong></h2>
<h3 id="artifact-cs-360-weight-tracker-android-application"><strong>Artifact: CS 360 Weight Tracker Android Application</strong></h3>
<h3 id="existing-code-functionality-2"><strong>Existing Code Functionality</strong></h3>
<p>Now I’ll move to the third artifact, the CS 360 Weight Tracker Android Application. This is a mobile application I developed that helps users track their weight over time and set weight goals.</p>
<p>The application uses SQLite, which is a lightweight relational database that stores data locally on the Android device. The current schema consists of two tables.</p>
<p>First, the Users table stores user account information. It has columns for user underscore id as the primary key, username which is unique, password underscore hash, current underscore weight, goal underscore weight, starting underscore weight, and created underscore at timestamp. This table maintains user authentication credentials and their weight goals.</p>
<p>Second, the Weight Entries table stores individual weight measurements. It has columns for entry underscore id as the primary key, user underscore id as a foreign key referencing the Users table, weight as a real number, date, time, and created underscore at timestamp. This table maintains a complete history of all weight entries for each user.</p>
<p>The application allows users to register and login with a username and password, record their weight each day with date and time stamps, view their weight history in a comprehensive list, set target weight goals with starting weight baseline, and see visual progress indicators showing percentage completion toward goals. All data is stored locally on the device using SQLite.</p>
<p>The typical query pattern involves getting a user and their weight entries, which looks like this: SELECT users.*, weight underscore entries.weight, weight underscore entries.date FROM users LEFT JOIN weight underscore entries ON users.user underscore id equals weight underscore entries.user underscore id WHERE users.username equals a specific username ORDER BY weight underscore entries.date DESC. This requires a JOIN operation every time we need to display a user’s data.</p>
<h3 id="code-analysis-2"><strong>Code Analysis</strong></h3>
<p>Now I’ll analyze this implementation to identify areas that need improvement.</p>
<p>Starting with structure, the relational schema with foreign keys is technically correct, but it’s not optimal for this particular use case. We’re using separate tables and requiring JOIN operations for what is essentially always accessed as a single unit - a user and their weight entries. The date field is stored as TEXT instead of using a proper date type. This makes date range queries less efficient and error-prone.</p>
<p>Now for the security issue.  I can see that passwords are stored using SHA-256 hashing, but this is insufficient for modern security standards. SHA-256 is a cryptographic hash function designed for speed, which actually makes it vulnerable to brute-force attacks. Modern password security requires algorithms specifically designed to be slow, like bcrypt, which makes brute-force attacks computationally expensive.</p>
<p>For performance, beyond the automatically created primary key indexes, there are no additional indexes defined. Queries that filter by date range or search for specific patterns would benefit from indexes. The JOIN operations required to get user data with weight entries add overhead. Every query must join two tables.</p>
<h3 id="planned-enhancements-2"><strong>Planned Enhancements</strong></h3>
<p>To address all these issues, I will migrate this application from SQLite to MongoDB with a complete schema redesign.</p>
<p>Instead of two separate tables, I’ll use a document-based design where each user is a single document containing all their related data. The structure looks like this: Each document has an underscore id field as an ObjectId, a username field, a password field containing a bcrypt hashed password, current underscore weight, goal underscore weight, and starting underscore weight fields, a created underscore at field as an ISODate, and a weight underscore entries array. Each element in the weight underscore entries array is an object with weight, date as ISODate, time, and optional notes fields.</p>
<p>For my first design decision on embedded documents versus references, I’m choosing to embed weight underscore entries within the user document rather than keeping them as a separate collection. Embedding is appropriate because weight entries are always accessed together with user data - we never query weight entries independently. The array size is bounded - even if someone weighs in daily, 365 entries per year is manageable. I can update a user and add a weight entry in a single database operation. It also improves performance - one document fetch instead of joining multiple tables.</p>
<p>For the critical security enhancement - password hashing using bcrypt - instead of SHA-256 hashing which is too fast, I’ll implement bcrypt hashing. Bcrypt is specifically designed for password hashing. It’s slow by design to make brute-force attacks computationally expensive, and it automatically handles salting to prevent rainbow table attacks. This is a significant improvement over SHA-256.</p>
<p>For my indexing strategy, I’ll create several indexes. First, a unique index on username for login lookups. Second, an index on weight underscore entries.date for date range queries. Third, a compound index on username and weight underscore entries.date for queries that filter by username and sort by date. The username index ensures login queries are fast instead of scanning all users. The weight entries date index enables efficient queries like “get last 30 days of data”. The compound index optimizes queries that filter by username and sort by date.</p>
<p>For connecting the Android app to MongoDB, I’ll add a simple REST API using Express.js. This will provide endpoints for the Android app to communicate with the MongoDB database.</p>
<p>For the migration process, I’ll follow an ETL approach - Extract, Transform, Load. First, I’ll extract data from SQLite by exporting Users and Weight Entries tables to JSON. Second, I’ll transform the data by hashing passwords with bcrypt, converting TEXT dates to ISODate objects, joining related tables into embedded documents by adding weight entries array to each user, and mapping SQLite INTEGER IDs to MongoDB ObjectIds. Third, I’ll load into MongoDB by inserting documents into users collection and creating indexes. Fourth, I’ll verify by comparing record counts, validating foreign key references became valid ObjectIds, and testing queries for correct results.</p>
<p>Let me show a query performance comparison. In SQLite before migration, to get a user with last 30 weight entries, I need to SELECT from users u LEFT JOIN weight underscore entries w ON u.user underscore id equals w.user underscore id WHERE u.username equals a specific username AND w.date is greater than or equal to 30 days ago ORDER BY w.date DESC. This requires table scans, JOIN operations, and multiple disk I/O operations.</p>
<p>In MongoDB after migration, I simply call db.users.findOne with username as the query, and in the projection, I use dollar slice on weight underscore entries to get negative 30, which returns the last 30 entries. This is a single index lookup on username, a single document fetch, and no JOIN required. This is 5 to 10 times faster than the SQLite approach.</p>
<h2 id="conclusion"><strong>Conclusion</strong></h2>
<p>To summarize today’s code review, I’ve identified significant opportunities for improvement across all three categories of computer science.</p>
<p>For Software Design and Engineering, the current  Dashboard is a functional but monolithic Python/Dash application with no authentication, minimal error handling, and tightly coupled architecture. By transforming it into a MERN stack application, I’ll demonstrate modern full-stack development with a RESTful API using Express.js, a component-based React frontend, JWT authentication with role-based access control, comprehensive input validation, and professional security practices following OWASP guidelines.</p>
<p>For Algorithms and Data Structures, the current dashboard uses inefficient linear search algorithms with O(n) complexity for all operations. By implementing a Binary Search Tree for O(log n) ID lookups and Hash Tables for O(1) breed filtering, I’ll achieve measurable performance improvements of 5 to 30 times depending on the operation.</p>
<p>For Databases, the current Weight Tracker application uses SHA-256 password hashing, which is insufficient for modern security, stores data in a local SQLite database with no backup capability, and requires JOIN operations for common queries. By migrating to MongoDB, I’ll demonstrate modern NoSQL design with embedded documents, implement bcrypt password hashing for proper security, create an indexing strategy for query optimization, and add a simple REST API to connect the Android app to MongoDB.</p>
<p>Thank you for watching my code review. I’m excited to implement these enhancements in the upcoming milestones and demonstrate these skills in my professional ePortfolio. I believe this work will effectively showcase my readiness to contribute as a  software engineer.</p>

    </div>
  </div>
</body>

</html>
